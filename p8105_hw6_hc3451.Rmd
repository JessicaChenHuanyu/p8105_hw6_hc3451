---
title: "Data Science I Homework 6"
author: "Huanyu Chen"
date: "`r Sys.Date()`"
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(MASS)
library(ggplot2)
library(modelr)
library(purrr)
```

# Problem 1
## Load and Clean Data
```{r, warning=FALSE}
homicide = read_csv("./homicide-data.csv")

homicide = homicide |>
  mutate(city_state = paste0(city,", ",state),
         solved = ifelse(disposition == "Closed by arrest",1,0)) |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO",
                            "Tulsa, AL")) |>
  filter(victim_race %in% c("White", "Black")) |>
  mutate(victim_age = as.numeric(victim_age))
```

## Baltimore
### Logistic Regression
```{r}
baltimore = homicide |>
  filter(city == "Baltimore")

model_baltimore = glm(solved ~ victim_age + victim_race + victim_sex,
                      data = baltimore, family = binomial())

model <- model_baltimore |>
  broom::tidy() |>
  knitr::kable(digits = 3)

model
save(model, file = "./Baltimore Logistic Regression.RData")
```

### Odds Ratio
```{r}
odd_ratio = model_baltimore |>
  broom::tidy() |>
  filter(term == 'victim_sexMale') |>
  summarise(estimate = estimate,
            odd_ratio = exp(estimate),
            odd_ratio_lower = exp(estimate - 1.96 * std.error),
            odd_ratio_upper = exp(estimate + 1.96 * std.error)
         ) |>
  knitr::kable(digits = 3)
odd_ratio
```

## Other Cities
```{r}
city_or_function <- function(citystate) {
  city_glm_or <- homicide %>%
    filter(city_state == citystate) %>%
    glm(solved ~ victim_sex + victim_race + victim_age, 
        family = binomial(), data = .) %>%
    broom::tidy() %>%
    filter(term == "victim_sexMale") %>%
    summarise(estimate = estimate,
              odd_ratio = exp(estimate),
              odd_ratio_lower = exp(estimate - 1.96 * std.error),
              odd_ratio_upper = exp(estimate + 1.96 * std.error)
    )
  
  return(city_glm_or)
}

result <- homicide %>%
  distinct(city_state) %>%
  pull(city_state) %>%
  map_df(city_or_function, .id = "city_state")

result %>%
  knitr::kable(digits = 3)
```

## Plot
```{r}
ggplot(result, aes(x = reorder(city_state, odd_ratio),
                          y = odd_ratio, fill = city_state)) +
  geom_point(aes(y = odd_ratio)) +
  geom_errorbar(aes(ymin = odd_ratio_lower, ymax = odd_ratio_upper)) +
  labs(title = "Estimated Odds Ratios and Confidence Intervals for Each City",
       x = "City", y = "Estimated Odds Ratio") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill = FALSE)
```

Therefore, we can get New York (city 30) has the lowest odds ratio between gender, while Albuquerque (city 1) has the highest odds ratio between gender.

# Problem 2
## Load Data
```{r, warning = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  dplyr::select(name, id, everything())

head(weather_df)
```

## Bootstrap
```{r, warning = FALSE}
result = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    weather_lm = map(strap, ~lm(tmax ~ tmin + prcp, data = .x)),
    weather_lm_r2 = map(weather_lm, broom::tidy),
    weather_lm_beta = map(weather_lm, broom::glance)
    ) |>
  unnest(weather_lm_r2, weather_lm_beta) |>
  dplyr::select(term, estimate, r.squared) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  rename(
    beta_0 = `(Intercept)`,
    beta_1 = tmin,
    beta_2 = prcp) |>
  summarise(r_squared = r.squared,
            log_beta1_2 = log(beta_1*beta_2))
  
head(result)
```

## Plot Distributions: r^2
```{r, warning = FALSE}
result |>
  ggplot(aes(x = r_squared)) +
  geom_histogram() +
  labs(title = "Distribution of r^2 Estimates",
       x = "r^2 estimates")
```

According to the graph, we find that the estimates for r\^2 are around 0.92, which means that `tmin` and `prcp` are good predictors of `tmax`.

## Plot Distributions: log(beta_1 * beta_2)
```{r, warning = FALSE}
result |>
  ggplot(aes(x = log_beta1_2)) +
  geom_histogram() +
  labs(title = "Distribution of log(beta_1 * beta_2) Estimates",
       x = "log(beta_1 * beta_2) estimates")
```

According to the graph, we find that the estimates for log(beta_1 \* beta_2) are left skewed, with most frequencies around -6.

## 95% Confidence Interval
```{r}
r2_ci = quantile(pull(result, r_squared),
                 probs = c(0.025,0.975))
r2_ci

log_beta12_ci = quantile(pull(result, log_beta1_2),
                         probs = c(0.025,0.975), na.rm = TRUE)
log_beta12_ci
```

# Problem 3

## Load and Clean Data
```{r}
birthweight = read.csv("./birthweight.csv")

birthweight <- birthweight |>
  janitor::clean_names() |>
  na.omit() |>
  mutate(babysex = case_when(babysex == 1 ~ 'male',
                             babysex == 2 ~ 'female'),
         frace = case_when(frace == 1 ~ "White",
                        frace == 2 ~ "Black",
                        frace == 3 ~ "Asian",
                        frace == 4 ~ "Puerto Rican",
                        frace == 8 ~ "Other",
                        frace == 9 ~ "Unknown"),
         malform = case_when(malform == 0 ~ 'absent',
                             malform == 1 ~ 'present'),
         mrace = case_when(mrace == 1 ~ "White",
                        mrace == 2 ~ "Black",
                        mrace == 3 ~ "Asian",
                        mrace == 4 ~ "Puerto Rican",
                        mrace == 8 ~ "Other")
         )
head(birthweight)
```

## Regression Model
The following code attempts to use backward stepwise regression.
```{r}
full_model = lm(bwt ~ ., data = birthweight) |>
  step(direction = "backward", trace = FALSE)

summary(full_model)
```

## Plot Residuals Against Fitted Values
```{r}
birthweight |>
    add_predictions(full_model) |>
    add_residuals(full_model) |>
    ggplot(aes(x = pred, y = resid)) +
    geom_point()  + 
    geom_smooth(method = "lm") + 
    labs(title = "Residuals Against Fitted Values", 
       x = "Fitted Values", 
       y = "Residuals")
```

# Compare Models
```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = birthweight)
summary(model_1)
model_2 = lm(bwt ~ bhead * blength * babysex, data = birthweight)
summary(model_2)
```

```{r}
cv =
  crossv_mc(birthweight, 100) |>
  mutate(rmse_fit_full = map2_dbl(map(train,~full_model),
                                  test, ~rmse(model = .x,data = .y)),
         rmse_fit_1 = map2_dbl(map(train,~model_1),
                               test, ~rmse(model = .x,data = .y)),
         rmse_fit_2 = map2_dbl(map(train,~model_2),
                               test, ~rmse(model = .x,data = .y))
  )
cv |>
  dplyr::select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |>
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

Therefore, we know that the 11-variable backward stepwise regression model has the smallest RMSE, indicating that the predictions in this model are closer to the actual values than the other two.